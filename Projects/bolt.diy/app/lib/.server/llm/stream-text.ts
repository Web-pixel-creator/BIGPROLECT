import { convertToCoreMessages, streamText as _streamText, type Message } from 'ai';
import * as fs from 'fs';
import * as path from 'path';
import { MAX_TOKENS, PROVIDER_COMPLETION_LIMITS, isReasoningModel, type FileMap } from './constants';
import { getSystemPrompt } from '~/lib/common/prompts/prompts';
import { DEFAULT_MODEL, DEFAULT_PROVIDER, MODIFICATIONS_TAG_NAME, PROVIDER_LIST, WORK_DIR } from '~/utils/constants';
import type { IProviderSetting } from '~/types/model';
import { PromptLibrary } from '~/lib/common/prompt-library';
import { allowedHTMLElements } from '~/utils/markdown';
import { LLMManager } from '~/lib/modules/llm/manager';
import { createScopedLogger } from '~/utils/logger';
import { createFilesContext, extractPropertiesFromMessage } from './utils';
import { discussPrompt } from '~/lib/common/prompts/discuss-prompt';
import type { DesignScheme } from '~/types/design-scheme';
import { registryService } from '~/lib/services/registryService';
import { EFFECT_PRESETS } from '~/lib/constants/promptPresets';
import { SmartComponentSelector } from '~/lib/services/smartComponentSelector';
import { StructuredPromptBuilder } from '~/lib/services/structuredPromptBuilder';

export type Messages = Message[];

export interface StreamingOptions extends Omit<Parameters<typeof _streamText>[0], 'model'> {
  supabaseConnection?: {
    isConnected: boolean;
    hasSelectedProject: boolean;
    credentials?: {
      anonKey?: string;
      supabaseUrl?: string;
    };
  };
}

const logger = createScopedLogger('stream-text');

function withTimeout<T>(promise: Promise<T>, ms: number, fallback: T): Promise<T> {
  return Promise.race([
    promise,
    new Promise<T>((resolve) => setTimeout(() => resolve(fallback), ms)),
  ]);
}

function getCompletionTokenLimit(modelDetails: any): number {
  // 1. If model specifies completion tokens, use that
  if (modelDetails.maxCompletionTokens && modelDetails.maxCompletionTokens > 0) {
    return modelDetails.maxCompletionTokens;
  }

  // 2. Use provider-specific default
  const providerDefault = PROVIDER_COMPLETION_LIMITS[modelDetails.provider];

  if (providerDefault) {
    return providerDefault;
  }

  // 3. Final fallback to MAX_TOKENS, but cap at reasonable limit for safety
  return Math.min(MAX_TOKENS, 16384);
}

function sanitizeText(text: string): string {
  let sanitized = text.replace(/<div class=\\"__boltThought__\\">.*?<\/div>/s, '');
  sanitized = sanitized.replace(/<think>.*?<\/think>/s, '');
  sanitized = sanitized.replace(/<boltAction type="file" filePath="package-lock\.json">[\s\S]*?<\/boltAction>/g, '');

  return sanitized.trim();
}

export async function streamText(props: {
  messages: Omit<Message, 'id'>[];
  env?: Env;
  options?: StreamingOptions;
  apiKeys?: Record<string, string>;
  files?: FileMap;
  providerSettings?: Record<string, IProviderSetting>;
  promptId?: string;
  contextOptimization?: boolean;
  contextFiles?: FileMap;
  summary?: string;
  messageSliceId?: number;
  chatMode?: 'discuss' | 'build';
  designScheme?: DesignScheme;
}) {
  const {
    messages,
    env: serverEnv,
    options,
    apiKeys,
    files,
    providerSettings,
    promptId,
    contextOptimization,
    contextFiles,
    summary,
    chatMode,
    designScheme,
  } = props;
  let currentModel = DEFAULT_MODEL;
  let currentProvider = DEFAULT_PROVIDER.name;
  let processedMessages = messages.map((message) => {
    const newMessage = { ...message };

    if (message.role === 'user') {
      const { model, provider, content } = extractPropertiesFromMessage(message);
      currentModel = model;
      currentProvider = provider;
      newMessage.content = sanitizeText(content);
    } else if (message.role == 'assistant') {
      newMessage.content = sanitizeText(message.content);
    }

    // Sanitize all text parts in parts array, if present
    if (Array.isArray(message.parts)) {
      newMessage.parts = message.parts.map((part) =>
        part.type === 'text' ? { ...part, text: sanitizeText(part.text) } : part,
      );
    }

    return newMessage;
  });

  const provider = PROVIDER_LIST.find((p) => p.name === currentProvider) || DEFAULT_PROVIDER;
  const staticModels = LLMManager.getInstance().getStaticModelListFromProvider(provider);
  let modelDetails = staticModels.find((m) => m.name === currentModel);

  if (!modelDetails) {
    const modelsList = [
      ...(provider.staticModels || []),
      ...(await LLMManager.getInstance().getModelListFromProvider(provider, {
        apiKeys,
        providerSettings,
        serverEnv: serverEnv as any,
      })),
    ];

    if (!modelsList.length) {
      throw new Error(`No models found for provider ${provider.name}`);
    }

    modelDetails = modelsList.find((m) => m.name === currentModel);

    if (!modelDetails) {
      // Check if it's a Google provider and the model name looks like it might be incorrect
      if (provider.name === 'Google' && currentModel.includes('2.5')) {
        throw new Error(
          `Model "${currentModel}" not found. Gemini 2.5 Pro doesn't exist. Available Gemini models include: gemini-1.5-pro, gemini-2.0-flash, gemini-1.5-flash. Please select a valid model.`,
        );
      }

      // Fallback to first model with warning
      logger.warn(
        `MODEL [${currentModel}] not found in provider [${provider.name}]. Falling back to first model. ${modelsList[0].name}`,
      );
      modelDetails = modelsList[0];
    }
  }

  const dynamicMaxTokens = modelDetails ? getCompletionTokenLimit(modelDetails) : Math.min(MAX_TOKENS, 16384);

  // Use model-specific limits directly - no artificial cap needed
  const safeMaxTokens = dynamicMaxTokens;

  logger.info(
    `Token limits for model ${modelDetails.name}: maxTokens=${safeMaxTokens}, maxTokenAllowed=${modelDetails.maxTokenAllowed}, maxCompletionTokens=${modelDetails.maxCompletionTokens}`,
  );

  let systemPrompt =
    PromptLibrary.getPropmtFromLibrary(promptId || 'default', {
      cwd: WORK_DIR,
      allowedHtmlElements: allowedHTMLElements,
      modificationTagName: MODIFICATIONS_TAG_NAME,
      designScheme,
      supabase: {
        isConnected: options?.supabaseConnection?.isConnected || false,
        hasSelectedProject: options?.supabaseConnection?.hasSelectedProject || false,
        credentials: options?.supabaseConnection?.credentials || undefined,
      },
    }) ?? getSystemPrompt();

  // Component selection - analyze user request and add relevant components
  try {
    // Get the last user message to analyze
    const lastUserMessage = processedMessages.filter((m) => m.role === 'user').pop();

    if (lastUserMessage && typeof lastUserMessage.content === 'string') {
      const userLower = lastUserMessage.content.toLowerCase();

      const requestedEffects = EFFECT_PRESETS.filter((e) => userLower.includes(e.label.toLowerCase())).map((e) =>
        e.label.toLowerCase(),
      );

      const selector = new SmartComponentSelector();
      const builder = new StructuredPromptBuilder();

      const selection = selector.select({
        theme: inferTheme(userLower),
        sections: inferSections(userLower),
        effects: requestedEffects,
        allow3d: allow3d(userLower),
      });

      if (selection.components.length > 0 || selection.effects.length > 0) {
        const componentContext = builder.build(selection, lastUserMessage.content);
        systemPrompt = `${systemPrompt}\n${componentContext}`;
        logger.info(
          `Added structured component context: comps=${selection.components.length}, effects=${selection.effects.length}`,
        );
        writeSelectionLog(selection, lastUserMessage.content);
      } else {
        logger.warn('No components selected for user message');
      }
    }
  } catch (error) {
    logger.warn('Failed to match components for prompt:', error);
  }


    // DEBUG selection summary
  // TOKEN GUARD for component context length
  const maxContextChars = 20000;
  if (selection && selection.components && selection.effects) {
    const dbgParts = (selection.components || []).concat(selection.effects || []);
    const approxContext = dbgParts.map((c) => c.code || '').join('\n');
    if (approxContext.length > maxContextChars) {
      const sorted = dbgParts.sort((a, b) => (b.score || 0) - (a.score || 0));
      let acc = '';
      const keep = [] as typeof sorted;
      for (const c of sorted) {
        const next = acc.length + (c.code || '').length;
        if (next > maxContextChars) continue;
        acc += c.code || '';
        keep.push(c);
      }
      const keepSet = new Set(keep.map((c) => `${c.name}__${c.source}`));
      selection.components = selection.components?.filter((c) => keepSet.has(`${c.name}__${c.source}`)) || [];
      selection.effects = selection.effects?.filter((c) => keepSet.has(`${c.name}__${c.source}`)) || [];
      selection.totalCodeLines = keep.reduce((s, c) => s + (c.code ? c.code.split(/\r?\n/).length : 0), 0);
      logger.warn(`Component context trimmed to ${keep.length} items (chars=${acc.length}) due to token guard`);
    }
  }

  const selectionDebug = selection?.debug || {};
  logger.info(
    'DEBUG COMPONENT SELECTION',
    {
      theme: selectionDebug.theme,
      sections: selectionDebug.sections,
      effects: selectionDebug.effects,
      components:
        selection?.components?.map((c) => ({ name: c.name, source: c.source, lines: c.code?.split(/\r?\n/).length || 0 })) ||
        [],
      effectsSelected:
        selection?.effects?.map((c) => ({ name: c.name, source: c.source, lines: c.code?.split(/\r?\n/).length || 0 })) ||
        [],
      totalLines: selection?.totalCodeLines,
      deps: selection?.dependencies,
    },
  );

}
